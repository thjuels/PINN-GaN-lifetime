{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2406aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2f960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_23 = pd.read_csv(\"data/0901_Tek023.csv\")\n",
    "data_interp = pd.read_csv(\"data/0901_Tek023_interpolated.csv\")\n",
    "\n",
    "print(\"Original data shape:\", data_23.shape)\n",
    "print(\"Interpolated data shape:\", data_interp.shape)\n",
    "data_23.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9c601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and parameters\n",
    "epsilon = 1e-9\n",
    "\n",
    "# Prepare data\n",
    "# Use interpolated data for training\n",
    "train_mask = data_interp[\"t\"] <= 1000\n",
    "test_mask = data_interp[\"t\"] > 1000\n",
    "\n",
    "t_train = data_interp[\"t\"][train_mask].values\n",
    "log_t_train = data_interp[\"log_t\"][train_mask].values\n",
    "vth_train = data_interp[\"vth_average\"][train_mask].values\n",
    "\n",
    "t_test = data_interp[\"t\"][test_mask].values\n",
    "log_t_test = data_interp[\"log_t\"][test_mask].values\n",
    "vth_test = data_interp[\"vth_average\"][test_mask].values\n",
    "\n",
    "print(f\"Training samples: {len(t_train)}\")\n",
    "print(f\"Test samples: {len(t_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea41089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "scaler_t = MinMaxScaler()\n",
    "scaler_vth = MinMaxScaler()\n",
    "\n",
    "log_t_train_scaled = scaler_t.fit_transform(log_t_train.reshape(-1, 1))\n",
    "log_t_test_scaled = scaler_t.transform(log_t_test.reshape(-1, 1))\n",
    "\n",
    "vth_train_scaled = scaler_vth.fit_transform(vth_train.reshape(-1, 1))\n",
    "vth_test_scaled = scaler_vth.transform(vth_test.reshape(-1, 1))\n",
    "\n",
    "# Convert to tensors\n",
    "t_train_tf = tf.constant(log_t_train_scaled, dtype=tf.float32)\n",
    "t_test_tf = tf.constant(log_t_test_scaled, dtype=tf.float32)\n",
    "vth_train_tf = tf.constant(vth_train_scaled, dtype=tf.float32)\n",
    "vth_test_tf = tf.constant(vth_test_scaled, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e0216b",
   "metadata": {},
   "source": [
    "## Physics Loss Function\n",
    "\n",
    "**TODO**: Define the physics-based loss function based on GaN device degradation mechanisms.\n",
    "\n",
    "Possible physics to incorporate:\n",
    "- Trap-assisted degradation: $\\frac{dV_{th}}{dt} = f(V_{th}, t, T, ...)$\n",
    "- Power-law degradation: $V_{th}(t) = V_{th,0} + A \\cdot t^n$\n",
    "- Logarithmic degradation: $V_{th}(t) = V_{th,0} + A \\cdot \\ln(1 + t/\\tau)$\n",
    "- Arrhenius temperature dependence\n",
    "- Interface trap dynamics\n",
    "\n",
    "The physics loss can enforce:\n",
    "1. Governing differential equations\n",
    "2. Boundary/initial conditions\n",
    "3. Physical constraints (e.g., monotonicity, bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f9c7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PHYSICS LOSS FUNCTION - TO BE MODIFIED\n",
    "# =============================================================================\n",
    "\n",
    "def physics_loss(model, t_input):\n",
    "    \"\"\"\n",
    "    Compute the physics-based loss term.\n",
    "    \n",
    "    This function should encode the physical laws governing Vth degradation.\n",
    "    Modify this function to incorporate the correct physics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : keras.Model\n",
    "        The neural network model\n",
    "    t_input : tf.Tensor\n",
    "        Time input tensor (scaled)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tf.Tensor\n",
    "        Physics loss value\n",
    "    \"\"\"\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(t_input)\n",
    "        vth_pred = model(t_input, training=True)\n",
    "    \n",
    "    # Compute dVth/dt (gradient of Vth with respect to time)\n",
    "    dvth_dt = tape.gradient(vth_pred, t_input)\n",
    "    \n",
    "    # =============================================================================\n",
    "    # PLACEHOLDER PHYSICS EQUATION - MODIFY THIS\n",
    "    # =============================================================================\n",
    "    # Example: Enforce that dVth/dt follows some physical law\n",
    "    # \n",
    "    # Option 1: Simple power-law constraint (dVth/dt ~ t^(n-1))\n",
    "    # Option 2: Logarithmic constraint (dVth/dt ~ 1/t)\n",
    "    # Option 3: Custom differential equation\n",
    "    #\n",
    "    # Current placeholder: Just penalize large derivatives (smoothness)\n",
    "    # Replace with actual physics equation when ready\n",
    "    \n",
    "    # Placeholder: smoothness regularization\n",
    "    physics_residual = dvth_dt  # Modify this to be the actual physics residual\n",
    "    \n",
    "    # Example physics residual for power-law: dVth/dt = A * n * t^(n-1)\n",
    "    # A = 0.01  # amplitude (to be fitted or set)\n",
    "    # n = 0.2   # exponent (to be fitted or set)\n",
    "    # expected_dvth_dt = A * n * tf.pow(t_input + epsilon, n - 1)\n",
    "    # physics_residual = dvth_dt - expected_dvth_dt\n",
    "    \n",
    "    # Return mean squared physics residual\n",
    "    return tf.reduce_mean(tf.square(physics_residual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdf6b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ADDITIONAL PHYSICS CONSTRAINTS - TO BE MODIFIED\n",
    "# =============================================================================\n",
    "\n",
    "def boundary_condition_loss(model, t_initial, vth_initial):\n",
    "    \"\"\"\n",
    "    Enforce initial/boundary conditions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : keras.Model\n",
    "        The neural network model\n",
    "    t_initial : tf.Tensor\n",
    "        Initial time point(s)\n",
    "    vth_initial : tf.Tensor\n",
    "        Known Vth at initial time\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tf.Tensor\n",
    "        Boundary condition loss\n",
    "    \"\"\"\n",
    "    vth_pred_initial = model(t_initial, training=True)\n",
    "    return tf.reduce_mean(tf.square(vth_pred_initial - vth_initial))\n",
    "\n",
    "\n",
    "def monotonicity_loss(model, t_input):\n",
    "    \"\"\"\n",
    "    Enforce monotonic increase/decrease of Vth over time (if physically expected).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : keras.Model\n",
    "        The neural network model\n",
    "    t_input : tf.Tensor\n",
    "        Time input tensor\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tf.Tensor\n",
    "        Monotonicity loss (penalizes negative derivatives if expecting increase)\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(t_input)\n",
    "        vth_pred = model(t_input, training=True)\n",
    "    \n",
    "    dvth_dt = tape.gradient(vth_pred, t_input)\n",
    "    \n",
    "    # Penalize negative derivatives (if Vth should increase)\n",
    "    # Use ReLU to only penalize negative values\n",
    "    violation = tf.nn.relu(-dvth_dt)  # Penalize if dvth_dt < 0\n",
    "    \n",
    "    return tf.reduce_mean(tf.square(violation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26f65af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PINN MODEL DEFINITION\n",
    "# =============================================================================\n",
    "\n",
    "def create_pinn_model(input_dim=1, hidden_layers=[64, 64, 32], output_dim=1):\n",
    "    \"\"\"\n",
    "    Create a neural network for PINN.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dim : int\n",
    "        Number of input features (typically 1 for time)\n",
    "    hidden_layers : list\n",
    "        List of hidden layer sizes\n",
    "    output_dim : int\n",
    "        Number of outputs (typically 1 for Vth)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    keras.Model\n",
    "        The PINN model\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=(input_dim,))\n",
    "    x = inputs\n",
    "    \n",
    "    for units in hidden_layers:\n",
    "        x = layers.Dense(units, activation='tanh')(x)  # tanh is common for PINNs\n",
    "    \n",
    "    outputs = layers.Dense(output_dim)(x)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "pinn_model = create_pinn_model(input_dim=1, hidden_layers=[64, 64, 32], output_dim=1)\n",
    "pinn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9323ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOSS WEIGHTS - ADJUST THESE\n",
    "# =============================================================================\n",
    "\n",
    "# Weight for data loss (fitting to measured data)\n",
    "lambda_data = 1.0\n",
    "\n",
    "# Weight for physics loss (enforcing physical equations)\n",
    "lambda_physics = 0.1  # Start small, increase if physics is important\n",
    "\n",
    "# Weight for boundary condition loss\n",
    "lambda_bc = 0.1\n",
    "\n",
    "# Weight for monotonicity loss (optional)\n",
    "lambda_mono = 0.0  # Set > 0 if Vth should be monotonic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd09592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CUSTOM TRAINING LOOP\n",
    "# =============================================================================\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "@tf.function\n",
    "def train_step(t_data, vth_data, t_collocation):\n",
    "    \"\"\"\n",
    "    Single training step for PINN.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    t_data : tf.Tensor\n",
    "        Time points with known Vth values\n",
    "    vth_data : tf.Tensor\n",
    "        Known Vth values at t_data\n",
    "    t_collocation : tf.Tensor\n",
    "        Collocation points for physics loss (can be same as t_data or denser)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary of loss values\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Data loss\n",
    "        vth_pred = pinn_model(t_data, training=True)\n",
    "        data_loss = tf.reduce_mean(tf.square(vth_pred - vth_data))\n",
    "        \n",
    "        # Physics loss\n",
    "        phys_loss = physics_loss(pinn_model, t_collocation)\n",
    "        \n",
    "        # Boundary condition loss (use first point as initial condition)\n",
    "        bc_loss = boundary_condition_loss(\n",
    "            pinn_model, \n",
    "            t_data[:1], \n",
    "            vth_data[:1]\n",
    "        )\n",
    "        \n",
    "        # Monotonicity loss (optional)\n",
    "        mono_loss = monotonicity_loss(pinn_model, t_collocation)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = (lambda_data * data_loss + \n",
    "                      lambda_physics * phys_loss + \n",
    "                      lambda_bc * bc_loss +\n",
    "                      lambda_mono * mono_loss)\n",
    "    \n",
    "    # Compute gradients and update weights\n",
    "    gradients = tape.gradient(total_loss, pinn_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, pinn_model.trainable_variables))\n",
    "    \n",
    "    return {\n",
    "        'total_loss': total_loss,\n",
    "        'data_loss': data_loss,\n",
    "        'physics_loss': phys_loss,\n",
    "        'bc_loss': bc_loss,\n",
    "        'mono_loss': mono_loss\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3b3cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "epochs = 5000\n",
    "print_every = 500\n",
    "\n",
    "# Create collocation points (can be denser than data points)\n",
    "# Using training data points as collocation points\n",
    "t_collocation = t_train_tf\n",
    "\n",
    "# Store loss history\n",
    "loss_history = {\n",
    "    'total_loss': [],\n",
    "    'data_loss': [],\n",
    "    'physics_loss': [],\n",
    "    'bc_loss': [],\n",
    "    'mono_loss': []\n",
    "}\n",
    "\n",
    "print(\"Starting PINN training...\")\n",
    "print(f\"Loss weights: data={lambda_data}, physics={lambda_physics}, bc={lambda_bc}, mono={lambda_mono}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    losses = train_step(t_train_tf, vth_train_tf, t_collocation)\n",
    "    \n",
    "    # Store losses\n",
    "    for key in loss_history:\n",
    "        loss_history[key].append(losses[key].numpy())\n",
    "    \n",
    "    if (epoch + 1) % print_every == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"  Total: {losses['total_loss']:.6f}, Data: {losses['data_loss']:.6f}, \"\n",
    "              f\"Physics: {losses['physics_loss']:.6f}, BC: {losses['bc_loss']:.6f}\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353c6954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "axes[0, 0].semilogy(loss_history['total_loss'])\n",
    "axes[0, 0].set_title('Total Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "\n",
    "axes[0, 1].semilogy(loss_history['data_loss'])\n",
    "axes[0, 1].set_title('Data Loss')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "\n",
    "axes[1, 0].semilogy(loss_history['physics_loss'])\n",
    "axes[1, 0].set_title('Physics Loss')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "\n",
    "axes[1, 1].semilogy(loss_history['bc_loss'])\n",
    "axes[1, 1].set_title('Boundary Condition Loss')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1938870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "# Predict on training and test data\n",
    "vth_pred_train_scaled = pinn_model.predict(t_train_tf)\n",
    "vth_pred_test_scaled = pinn_model.predict(t_test_tf)\n",
    "\n",
    "# Inverse transform to original scale\n",
    "vth_pred_train = scaler_vth.inverse_transform(vth_pred_train_scaled).flatten()\n",
    "vth_pred_test = scaler_vth.inverse_transform(vth_pred_test_scaled).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "def calculate_metrics(y_true, y_pred, name=\"Model\"):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  MSE:  {mse:.6f}\")\n",
    "    print(f\"  RMSE: {rmse:.6f}\")\n",
    "    print(f\"  MAE:  {mae:.6f}\")\n",
    "    print(f\"  RÂ²:   {r2:.6f}\")\n",
    "    \n",
    "    return {\"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae, \"R2\": r2}\n",
    "\n",
    "print(\"=== PINN Model Evaluation ===\")\n",
    "train_metrics = calculate_metrics(vth_train, vth_pred_train, \"Training Set\")\n",
    "test_metrics = calculate_metrics(vth_test, vth_pred_test, \"Test Set (t > 1000s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35ce4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actual\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Training range\n",
    "axes[0].plot(log_t_train, vth_train, 'k-', linewidth=2, label=\"Actual\")\n",
    "axes[0].plot(log_t_train, vth_pred_train, 'r--', alpha=0.8, label=\"PINN\")\n",
    "axes[0].set_xlabel(\"log(Time)\")\n",
    "axes[0].set_ylabel(\"Vth\")\n",
    "axes[0].set_title(\"Training Range (t <= 1000s)\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Test range\n",
    "axes[1].plot(log_t_test, vth_test, 'k-', linewidth=2, label=\"Actual\")\n",
    "axes[1].plot(log_t_test, vth_pred_test, 'r--', alpha=0.8, label=\"PINN\")\n",
    "axes[1].set_xlabel(\"log(Time)\")\n",
    "axes[1].set_ylabel(\"Vth\")\n",
    "axes[1].set_title(\"Test Range (t > 1000s)\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb41c2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full range plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Combine all data\n",
    "all_log_t = np.concatenate([log_t_train, log_t_test])\n",
    "all_vth_actual = np.concatenate([vth_train, vth_test])\n",
    "all_vth_pred = np.concatenate([vth_pred_train, vth_pred_test])\n",
    "\n",
    "plt.plot(all_log_t, all_vth_actual, 'k-', linewidth=2, label=\"Actual\")\n",
    "plt.plot(all_log_t, all_vth_pred, 'r--', alpha=0.8, label=\"PINN Prediction\")\n",
    "plt.axvline(x=np.log(1000), color='gray', linestyle=':', label=\"Train/Test Split (t=1000s)\")\n",
    "plt.xlabel(\"log(Time)\")\n",
    "plt.ylabel(\"Vth\")\n",
    "plt.title(\"PINN: Full Range Prediction\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8245a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PREDICT AT SPECIFIC TIME (10000s)\n",
    "# =============================================================================\n",
    "\n",
    "# Find 10000s in original data\n",
    "idx_10000 = (np.abs(data_23[\"t\"] - 10000)).idxmin()\n",
    "actual_vth_10000 = data_23[\"vth_average\"][idx_10000]\n",
    "log_t_10000 = np.log(data_23[\"t\"][idx_10000] + epsilon)\n",
    "\n",
    "# Scale and predict\n",
    "log_t_10000_scaled = scaler_t.transform([[log_t_10000]])\n",
    "vth_pred_10000_scaled = pinn_model.predict(log_t_10000_scaled)\n",
    "vth_pred_10000 = scaler_vth.inverse_transform(vth_pred_10000_scaled)[0, 0]\n",
    "\n",
    "print(f\"\\n=== Prediction at t=10000s ===\")\n",
    "print(f\"Actual Vth:    {actual_vth_10000:.6f}\")\n",
    "print(f\"PINN Predicted: {vth_pred_10000:.6f}\")\n",
    "print(f\"Error:         {abs(actual_vth_10000 - vth_pred_10000):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c1e508",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Modify `physics_loss()` function** with the correct physical equations for GaN Vth degradation\n",
    "2. **Tune loss weights** (`lambda_data`, `lambda_physics`, `lambda_bc`, `lambda_mono`)\n",
    "3. **Add more physics constraints** if needed (temperature dependence, stress conditions, etc.)\n",
    "4. **Compare with pure data-driven models** to see if physics helps extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7434ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TEMPLATE FOR CUSTOM PHYSICS LOSS\n",
    "# =============================================================================\n",
    "\n",
    "# Uncomment and modify the physics loss function below when ready\n",
    "\n",
    "# def physics_loss_custom(model, t_input):\n",
    "#     \"\"\"\n",
    "#     Custom physics loss based on GaN degradation mechanism.\n",
    "#     \n",
    "#     Example: Power-law degradation\n",
    "#     Vth(t) = Vth0 + A * t^n\n",
    "#     => dVth/dt = A * n * t^(n-1)\n",
    "#     \"\"\"\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         tape.watch(t_input)\n",
    "#         vth_pred = model(t_input, training=True)\n",
    "#     \n",
    "#     dvth_dt = tape.gradient(vth_pred, t_input)\n",
    "#     \n",
    "#     # Physical parameters (to be determined from experiments or fitting)\n",
    "#     A = 0.01   # Amplitude coefficient\n",
    "#     n = 0.2    # Power-law exponent\n",
    "#     \n",
    "#     # Expected derivative from physics\n",
    "#     # Note: t_input is scaled, need to transform back or use scaled equation\n",
    "#     expected_dvth_dt = A * n * tf.pow(t_input + epsilon, n - 1)\n",
    "#     \n",
    "#     # Physics residual\n",
    "#     residual = dvth_dt - expected_dvth_dt\n",
    "#     \n",
    "#     return tf.reduce_mean(tf.square(residual))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
